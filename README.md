<h1>MNIST</h1>
<p>Project dependencies: PyTorch, Sklearn, Numpy, PIL, Matplotlib, and gzip.</p>
<ul>
    <li><a href="https://github.com/AgamChopra/MNIST/blob/main/my_dataset.py">my_dataset.py</a>: Contains functions to load MNIST dataset into numpy format, convert custom digits into MNIST
        format, and function to visualize dataset.
        Download complete MNIST dataset <a href="http://yann.lecun.com/exdb/mnist/" target="blank">here</a>.
    <li><a href="https://github.com/AgamChopra/MNIST/blob/main/models.py">models.py</a>: Cantains various custom ML models and helper functions.
    <li><a href="https://github.com/AgamChopra/MNIST/blob/main/main.py">main.py</a>: Executable file for training the models on the MNIST dataset.
    <li><a href="https://github.com/AgamChopra/MNIST/tree/main/my_numbers">my_numbers</a>: File containing my handdrawn digits [0,9].
</ul>
<p align="center">
    <img width="680" height="80" src="https://github.com/AgamChopra/MNIST/blob/main/misc_imgs/stats_.PNG?raw=true">
    <br><i>Fig 1. Observed Accuracy after 100 Iterations.</i><br><br>
    <img width="280" height="200"
        src="https://github.com/AgamChopra/MNIST/blob/main/misc_imgs/NN_1_neuron.png?raw=true">
    <br><i>Fig 2. Loss Plots for Perceptron</i><br>
</p>

<h2>Future Work</h2>
<p><strike>Implement a Vision Transformer model </strike><a href="https://arxiv.org/pdf/1706.03762.pdf" target="blank">[A. Vaswani
        et al., 2017]</a><a href="https://arxiv.org/pdf/2010.11929.pdf" target="blank">[A. Dosovitskiy et al., 2021]</a><br>
Implement Focal Self-attention for Local-Global Interactions in Vision Transformers <a href="https://arxiv.org/pdf/2107.00641.pdf" target="blank">[J. Yang et al., 2021]</a></p>
<h2>Lisence</h2>
<p><a href="https://raw.githubusercontent.com/AgamChopra/MNIST/main/LICENSE?token=AFTUZ6KEH5IE4L4ZIZCCUF3BYTS6C" target="blank">[The MIT License]</a></p>
